{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j0LHvmCrahfW"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from  llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import (\n",
        "    SentenceSplitter,\n",
        "    SemanticSplitterNodeParser,\n",
        ")\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core import VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K4MVubW7a46N"
      },
      "outputs": [],
      "source": [
        "#model is loaded from ollama model phi3.5\n",
        "llm = Ollama(model=\"phi3.5\", request_timeout=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#setting up the embedding model from huggingface\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oaFjq8iEbGGa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id_='7d5baed2-d74f-4a07-a3eb-6bd433e0277c', embedding=None, metadata={'page_label': '1', 'file_name': 'cheatsheet-unsupervised-learning.pdf', 'file_path': 'c:\\\\Users\\\\ziade\\\\Desktop\\\\eeeeee\\\\data\\\\cheatsheet-unsupervised-learning.pdf', 'file_type': 'application/pdf', 'file_size': 456557, 'creation_date': '2024-09-11', 'last_modified_date': '2024-09-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CS 229 – Machine Learning https://stanford.edu/~shervine\\nVIP Cheatsheet: Unsupervised Learning\\nAfshineAmidi and Shervine Amidi\\nSeptember 9, 2018\\nIntroduction to Unsupervised Learning\\nÌMotivation – The goal of unsupervised learning is to ﬁnd hidden patterns in unlabeled data\\n{x(1),...,x(m)}.\\nÌJensen’s inequality – Letfbe a convex function and Xa random variable. We have the\\nfollowing inequality:\\nE[f(X)]⩾f(E[X])\\nExpectation-Maximization\\nÌLatent variables – Latent variables are hidden/unobserved variables that make estimation\\nproblems diﬃcult, and are often denoted z. Here are the most common settings where there are\\nlatent variables:\\nSetting Latent variable zx|z Comments\\nMixture of kGaussians Multinomial (φ)N(µj,Σj)µj∈Rn,φ∈Rk\\nFactor analysis N(0,I)N(µ+ Λz,ψ)µj∈Rn\\nÌAlgorithm – The Expectation-Maximization (EM) algorithm gives an eﬃcient method at\\nestimating the parameter θthrough maximum likelihood estimation by repeatedly constructing\\na lower-bound on the likelihood (E-step) and optimizing that lower bound (M-step) as follows:\\n•E-step: Evaluate the posterior probability Qi(z(i))that each data point x(i)came from\\na particular cluster z(i)as follows:\\nQi(z(i)) =P(z(i)|x(i);θ)\\n•M-step: Use the posterior probabilities Qi(z(i))as cluster speciﬁc weights on data points\\nx(i)to separately re-estimate each cluster model as follows:\\nθi=argmax\\nθ∑\\niˆ\\nz(i)Qi(z(i)) log(\\nP(x(i),z(i);θ)\\nQi(z(i)))\\ndz(i)\\nk-means clustering\\nWe notec(i)the cluster of data point iandµjthe center of cluster j.\\nÌAlgorithm – After randomly initializing the cluster centroids µ1,µ2,...,µk∈Rn, thek-means\\nalgorithm repeats the following step until convergence:\\nc(i)=arg min\\nj||x(i)−µj||2andµj=m∑\\ni=11{c(i)=j}x(i)\\nm∑\\ni=11{c(i)=j}\\nÌDistortion function – In order to see if the algorithm converges, we look at the distortion\\nfunction deﬁned as follows:\\nJ(c,µ) =m∑\\ni=1||x(i)−µc(i)||2\\nHierarchical clustering\\nÌAlgorithm – It is a clustering algorithm with an agglomerative hierarchical approach that\\nbuild nested clusters in a successive manner.\\nÌTypes– There are diﬀerent sorts of hierarchical clustering algorithms that aims at optimizing\\ndiﬀerent objective functions, which is summed up in the table below:\\nStanford University 1 Fall 2018', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7cf3442e-d885-4671-871f-c257cbf732ce', embedding=None, metadata={'page_label': '2', 'file_name': 'cheatsheet-unsupervised-learning.pdf', 'file_path': 'c:\\\\Users\\\\ziade\\\\Desktop\\\\eeeeee\\\\data\\\\cheatsheet-unsupervised-learning.pdf', 'file_type': 'application/pdf', 'file_size': 456557, 'creation_date': '2024-09-11', 'last_modified_date': '2024-09-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CS 229 – Machine Learning https://stanford.edu/~shervine\\nWard linkage Average linkage Complete linkage\\nMinimize within cluster Minimize average distance Minimize maximum distance\\ndistance between cluster pairs of between cluster pairs\\nClustering assessment metrics\\nIn an unsupervised learning setting, it is often hard to assess the performance of a model since\\nwe don’t have the ground truth labels as was the case in the supervised learning setting.\\nÌSilhouette coeﬃcient – By noting aandbthe mean distance between a sample and all\\nother points in the same class, and between a sample and all other points in the next nearest\\ncluster, the silhouette coeﬃcient sfor a single sample is deﬁned as follows:\\ns=b−a\\nmax(a,b)\\nÌCalinski-Harabaz index – By noting kthe number of clusters, BkandWkthe between\\nand within-clustering dispersion matrices respectively deﬁned as\\nBk=k∑\\nj=1nc(i)(µc(i)−µ)(µc(i)−µ)T, Wk=m∑\\ni=1(x(i)−µc(i))(x(i)−µc(i))T\\nthe Calinski-Harabaz index s(k)indicates how well a clustering model deﬁnes its clusters, such\\nthat the higher the score, the more dense and well separated the clusters are. It is deﬁned as\\nfollows:\\ns(k) =Tr(Bk)\\nTr(Wk)×N−k\\nk−1\\nPrincipal component analysis\\nIt is a dimension reduction technique that ﬁnds the variance maximizing directions onto which\\nto project the data.\\nÌEigenvalue, eigenvector – Given a matrix A∈Rn×n,λis said to be an eigenvalue of Aif\\nthere exists a vector z∈Rn\\\\{0}, called eigenvector, such that we have:\\nAz=λz\\nÌSpectral theorem – LetA∈Rn×n. IfAis symmetric, then Ais diagonalizable by a real\\northogonal matrix U∈Rn×n. By noting Λ =diag(λ1,...,λn), we have:\\n∃Λdiagonal, A =UΛUT\\nRemark: the eigenvector associated with the largest eigenvalue is called principal eigenvector of\\nmatrixA.\\nÌAlgorithm – The Principal Component Analysis (PCA) procedure is a dimension reduction\\ntechnique that projects the data on kdimensions by maximizing the variance of the data as\\nfollows:•Step 1: Normalize the data to have a mean of 0 and standard deviation of 1.\\nx(i)\\nj←x(i)\\nj−µj\\nσjwhereµj=1\\nmm∑\\ni=1x(i)\\njandσ2\\nj=1\\nmm∑\\ni=1(x(i)\\nj−µj)2\\n•Step 2: Compute Σ =1\\nmm∑\\ni=1x(i)x(i)T∈Rn×n, whichissymmetricwithrealeigenvalues.\\n•Step 3: Compute u1,...,uk∈Rnthekorthogonal principal eigenvectors of Σ, i.e. the\\northogonal eigenvectors of the klargest eigenvalues.\\n•Step 4: Project the data on spanR(u1,...,uk). This procedure maximizes the variance\\namong allk-dimensional spaces.\\nIndependent component analysis\\nIt is a technique meant to ﬁnd the underlying generating sources.\\nÌAssumptions – We assume that our data xhas been generated by the n-dimensional source\\nvectors= (s1,...,sn), wheresiare independent random variables, via a mixing and non-singular\\nmatrixAas follows:\\nx=As\\nThe goal is to ﬁnd the unmixing matrix W=A−1by an update rule.\\nÌBell and Sejnowski ICA algorithm – This algorithm ﬁnds the unmixing matrix Wby\\nfollowing the steps below:\\n•Write the probability of x=As=W−1sas:\\np(x) =n∏\\ni=1ps(wT\\nix)·|W|\\n•Write the log likelihood given our training data {x(i),i∈[ [1,m] ]}and by noting gthe\\nsigmoid function as:\\nl(W) =m∑\\ni=1(n∑\\nj=1log(\\ng′(wT\\njx(i)))\\n+ log|W|)\\nStanford University 2 Fall 2018', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='565443e8-47ad-4b9a-b61a-48c16ec6806f', embedding=None, metadata={'page_label': '3', 'file_name': 'cheatsheet-unsupervised-learning.pdf', 'file_path': 'c:\\\\Users\\\\ziade\\\\Desktop\\\\eeeeee\\\\data\\\\cheatsheet-unsupervised-learning.pdf', 'file_type': 'application/pdf', 'file_size': 456557, 'creation_date': '2024-09-11', 'last_modified_date': '2024-09-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CS 229 – Machine Learning https://stanford.edu/~shervine\\nTherefore, the stochastic gradient ascent learning rule is such that for each training example\\nx(i), we update Was follows:\\nW←−W+α\\uf8eb\\n)\\uf8ed\\uf8eb\\n)\\uf8ed1−2g(wT\\n1x(i))\\n1−2g(wT\\n2x(i))\\n...\\n1−2g(wT\\nnx(i))(\\n\\uf8f7(x(i)T+ (WT)−1(\\n\\uf8f7(\\nStanford University 3 Fall 2018', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ],
      "source": [
        "#loading the data\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#splitting the document into nodes\n",
        "splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# also baseline splitter\n",
        "base_splitter = SentenceSplitter(chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#affecting nodes variable to the splitted document\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÌJensen’s inequality – Letfbe a convex function and Xa random variable. We have the\n",
            "following inequality:\n",
            "E[f(X)]⩾f(E[X])\n",
            "Expectation-Maximization\n",
            "ÌLatent variables – Latent variables are hidden/unobserved variables that make estimation\n",
            "problems diﬃcult, and are often denoted z. Here are the most common settings where there are\n",
            "latent variables:\n",
            "Setting Latent variable zx|z Comments\n",
            "Mixture of kGaussians Multinomial (φ)N(µj,Σj)µj∈Rn,φ∈Rk\n",
            "Factor analysis N(0,I)N(µ+ Λz,ψ)µj∈Rn\n",
            "ÌAlgorithm – The Expectation-Maximization (EM) algorithm gives an eﬃcient method at\n",
            "estimating the parameter θthrough maximum likelihood estimation by repeatedly constructing\n",
            "a lower-bound on the likelihood (E-step) and optimizing that lower bound (M-step) as follows:\n",
            "•E-step: Evaluate the posterior probability Qi(z(i))that each data point x(i)came from\n",
            "a particular cluster z(i)as follows:\n",
            "Qi(z(i)) =P(z(i)|x(i);θ)\n",
            "•M-step: Use the posterior probabilities Qi(z(i))as cluster speciﬁc weights on data points\n",
            "x(i)to separately re-estimate each cluster model as follows:\n",
            "θi=argmax\n",
            "θ∑\n",
            "iˆ\n",
            "z(i)Qi(z(i)) log(\n",
            "P(x(i),z(i);θ)\n",
            "Qi(z(i)))\n",
            "dz(i)\n",
            "k-means clustering\n",
            "We notec(i)the cluster of data point iandµjthe center of cluster j.\n",
            "ÌAlgorithm – After randomly initializing the cluster centroids µ1,µ2,...,µk∈Rn, thek-means\n",
            "algorithm repeats the following step until convergence:\n",
            "c(i)=arg min\n",
            "j||x(i)−µj||2andµj=m∑\n",
            "i=11{c(i)=j}x(i)\n",
            "m∑\n",
            "i=11{c(i)=j}\n",
            "ÌDistortion function – In order to see if the algorithm converges, we look at the distortion\n",
            "function deﬁned as follows:\n",
            "J(c,µ) =m∑\n",
            "i=1||x(i)−µc(i)||2\n",
            "Hierarchical clustering\n",
            "ÌAlgorithm – It is a clustering algorithm with an agglomerative hierarchical approach that\n",
            "build nested clusters in a successive manner.\n",
            "ÌTypes– There are diﬀerent sorts of hierarchical clustering algorithms that aims at optimizing\n",
            "diﬀerent objective functions, which is summed up in the table below:\n",
            "Stanford University 1 Fall 2018\n"
          ]
        }
      ],
      "source": [
        "print(nodes[1].get_content())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#indexing nodes using vector store index\n",
        "index = VectorStoreIndex(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#stored the indexed nodes in the storage context for later use to\n",
        "# avoid reindexing\n",
        "index.storage_context.persist(persist_dir=\".\\emb_storage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nreloading sample code\\nfrom llama_index.core import StorageContext, load_index_from_storage\\n\\n# rebuild storage context\\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\\n\\n# load index\\nindex = load_index_from_storage(storage_context)\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#used to override the default retriever which is from OPENAI\n",
        "Settings.llm = llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=10,\n",
        ")\n",
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.6)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = query_engine.query(\"what are the types of hierarchichal clustering\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
